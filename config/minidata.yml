Description: " train on small data set from 20 sample"
name_exp: "dlib"

#data_path: "/home/falhamdoosh/tgcn/data/PartA/Mediapipe/dataset_mediapipe_without_process.npy"
#labels_path: "/home/falhamdoosh/tgcn/data/PartA/Mediapipe/label_mediapipe.npy"
#edges_path: "/home/falhamdoosh/tgcn/data/PartA/Mediapipe/edges.npy"
data_path: "/home/falhamdoosh/tgcn/data/PartA/dlib_student/dataset_data_biovid.npy"
labels_path: "/home/falhamdoosh/tgcn/data/PartA/dlib_student/dataset_label_biovid.pkl"
edges_path: "/home/falhamdoosh/tgcn/data/PartA/dlib_student/edge_index_51biovid.npy"
video_path: "/home/falhamdoosh/tgcn/data/PartA/"
csv_file: "/home/falhamdoosh/tgcn/data/PartA/samples.csv"
idx_train: "/home/falhamdoosh/tgcn/data/PartA/minidata/idx_train.npy"
idx_test: "/home/falhamdoosh/tgcn/data/PartA/minidata/idx_test.npy"
###Optimizer###

weight_decay: 0.0001
step_decay: 10
optimizer: 'Adam'
#momentum: 0.9

###Network###

num_epoch: 500 
batch_size: 5
continue_training: False   #If set True will continue training from pretrained model
pretrain_model: ""       #Name of folder that contains the model resume training
gpu: 1
#k_fold: 3
###Model parameters###

embed_dim: 128
n_classes: 5
num_features: 4
output_features: 1
TS: 137
n_joints: 51
train_ratio: 0.7
lr: 0.001
LOG_DIR: "./log/"
#n_layers: 3
#dropout: 0.2